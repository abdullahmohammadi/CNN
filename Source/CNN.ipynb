{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrFreQQS7JTM"
      },
      "outputs": [],
      "source": [
        "class Convolution:\n",
        "    def __init__(self, input_shape, filter_size, num_filters):\n",
        "        input_height, input_width = input_shape\n",
        "        self.num_filters = num_filters\n",
        "        self.input_shape = input_shape\n",
        "        # Size of outputs and filters or kernels or weights\n",
        "        self.filter_size = filter_size\n",
        "        self.filter_shape = (num_filters, filter_size, filter_size)\n",
        "        self.output_shape = (num_filters, input_height - filter_size + 1, input_width - filter_size + 1)\n",
        "\n",
        "        # Initialize filters and biases\n",
        "        self.filters = self.random_normal(self.filter_shape)\n",
        "        self.biases = self.random_normal(self.output_shape)\n",
        "        #print(\"shape0: \", self.filter_shape, self.filters.shape)\n",
        "\n",
        "    def random_normal(self, shape):\n",
        "        import random\n",
        "        matrix = []\n",
        "        for _ in range(shape[0]):\n",
        "            row = []\n",
        "            for _ in range(shape[1]):\n",
        "                column = []\n",
        "                for _ in range(shape[2]):\n",
        "                    value = 0.1 * (2 * random.random() - 1)\n",
        "                    column.append(value)\n",
        "                row.append(column)\n",
        "            matrix.append(row)\n",
        "        return matrix\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        # Initialize the input value\n",
        "        #print(\"output shape: \", self.output_shape, len(self.output_shape), self.output_shape[0])\n",
        "        output = self.zeros(self.output_shape)\n",
        "        for i in range(self.num_filters):\n",
        "            #print(\"filtershape forward: \", self.filters[i].shape)\n",
        "            output[i] = self.correlate2d(input_data, self.filters[i], self.filter_size, mode=\"valid\")\n",
        "            # Applying Relu Activation function\n",
        "            output[i] = self.maximum(output[i], 0, self.output_shape[1])\n",
        "        return output\n",
        "\n",
        "    def backward(self, dL_dout, lr):\n",
        "        # Create a random dL_dout array to accommodate output gradients\n",
        "        dL_dinput = self.zeros(self.input_shape)\n",
        "        dL_dfilters = self.zeros(self.filter_shape)\n",
        "\n",
        "        for i in range(self.num_filters):\n",
        "            # Calculating the gradient of loss with respect to kernels\n",
        "            dL_dfilters[i] = self.correlate2d2(self.input_data, dL_dout[i], self.filter_size, mode=\"valid\")\n",
        "\n",
        "            # Calculating the gradient of loss with respect to inputs\n",
        "            #dL_dinput += self.correlate2d(dL_dout[i], self.filters[i], mode=\"full\")\n",
        "\n",
        "            #print(\"weights of filters:\", self.filters)\n",
        "            # Updating the parameters with learning rate\n",
        "            self.filters = self.update(self.filters, dL_dfilters[i], lr)\n",
        "            #self.biases = self.update(self.biases, dL_dout[i], lr)\n",
        "            #print(\"weights of filters:\", self.filters)\n",
        "\n",
        "        # Returning the gradient of inputs\n",
        "        return dL_dinput\n",
        "\n",
        "    def correlate2d2(self, input_data, kernel, size, mode):\n",
        "        #print(\"shape1: \", input_data.shape)\n",
        "        #print(\"klernel: \", len(kernel), len(kernel[0]))\n",
        "        #print(\"inputdtat_ shape: \", input_data.shape[0] - size + 1, input_data.shape[1] - size + 1)\n",
        "        output = self.zeros((size, size))\n",
        "        output1 = self.zeros((size, size))\n",
        "        #print(\"ouyput cor: \", output)\n",
        "        #print(\"HIIIIIII\")\n",
        "        for i in range(input_data.shape[0] - size + 1):\n",
        "            for j in range(input_data.shape[1] - size + 1):\n",
        "                output = self.multiply1( input_data[i:i+size, j:j+size], kernel[i][j])\n",
        "                #print(\"covnpotputy:\", output)\n",
        "                for m in range(size):\n",
        "                  for n in range(size):\n",
        "                    output1[m][n] += output[m][n]\n",
        "        #print(\"convoutput:\", output1)\n",
        "        return output1\n",
        "\n",
        "    def multiply1(self, mat1, scal):\n",
        "      output1 = self.zeros((self.filter_size, self.filter_size))\n",
        "      for i in range(len(mat1)):\n",
        "        for j in range(len(mat1[0])):\n",
        "          output1[i][j] = mat1[i][j] * scal\n",
        "      return output1\n",
        "\n",
        "    def correlate2d(self, input_data, kernel, size, mode):\n",
        "        #print(\"shape1: \", input_data.shape)\n",
        "        #print(\"shape2: \", kernel.shape)\n",
        "        #print(\"inputdtat_ shape: \", input_data.shape[0] - size + 1, input_data.shape[1] - size + 1)\n",
        "        output = self.zeros((input_data.shape[0] - size + 1, input_data.shape[1] - size + 1))\n",
        "        #print(\"ouyput cor: \", output)\n",
        "        #print(\"HIIIIIII\")\n",
        "        for i in range(input_data.shape[0] - size + 1):\n",
        "            for j in range(input_data.shape[1] - size + 1):\n",
        "                output[i][j] = self.sum(self.multiply(input_data[i:i+size, j:j+size], kernel), size)\n",
        "        return output\n",
        "\n",
        "    def maximum(self, arr, val, size):\n",
        "        #print(\"maximum!!!\")\n",
        "        #print(size)\n",
        "        output = [[0 for _ in range(size)] for _ in range(size)]\n",
        "        for i in range(size):\n",
        "            for j in range(size):\n",
        "                output[i][j] = max(arr[i][j], val)\n",
        "        return output\n",
        "\n",
        "    def multiply(self, arr1, arr2):\n",
        "        #print(\"multiply!!!\")\n",
        "        #print(\"multi shape: \", arr1.shape, len(arr1), len(arr1.shape), arr1)\n",
        "        output = self.zeros(arr1.shape)\n",
        "        for i in range(arr1.shape[0]):\n",
        "            for j in range(arr1.shape[1]):\n",
        "                output[i][j] = arr1[i][j] * arr2[i][j]\n",
        "        return output\n",
        "\n",
        "    def sum(self, arr, size):\n",
        "        output = 0\n",
        "        for i in range(size):\n",
        "            for j in range(size):\n",
        "                output += arr[i][j]\n",
        "        return output\n",
        "\n",
        "    def update(self, arr, grad, lr):\n",
        "        #output = self.zeros(arr.shape)\n",
        "            for j in range(len(arr)):\n",
        "                for k in range(len(arr[0])):\n",
        "                    arr[j][k] = arr[j][k] - lr * grad[j][k]\n",
        "            return arr\n",
        "\n",
        "    def zeros(self, shape):\n",
        "        #print( shape[1], shape[0], len(shape))\n",
        "        if len(shape)<3:\n",
        "          return [[0 for _ in range(shape[1])] for _ in range(shape[0])]\n",
        "        else:\n",
        "          return [[[0 for _ in range(shape[2])] for _ in range(shape[1])] for _ in range(shape[0])]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool:\n",
        "    def __init__(self, pool_size, kersize, input_hight, input_width):\n",
        "        self.pool_size = pool_size\n",
        "        self.num_channels = kersize\n",
        "        self.input_height = input_hight\n",
        "        self.input_width = input_width\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        self.output_height = self.input_height // self.pool_size\n",
        "        self.output_width = self.input_width // self.pool_size\n",
        "\n",
        "        # Determining the output shape\n",
        "        self.output = self.zeros((self.num_channels, self.output_height, self.output_width))\n",
        "\n",
        "        # Looping through different channels\n",
        "        for c in range(self.num_channels):\n",
        "            # Looping through the height\n",
        "            for i in range(self.output_height):\n",
        "                # Looping through the width\n",
        "                for j in range(self.output_width):\n",
        "                    # Starting position\n",
        "                    start_i = i * self.pool_size\n",
        "                    start_j = j * self.pool_size\n",
        "\n",
        "                    # Ending Position\n",
        "                    end_i = start_i + self.pool_size\n",
        "                    end_j = start_j + self.pool_size\n",
        "\n",
        "                    # Creating a patch from the input data\n",
        "                    patch = self.get_patch(input_data, c, start_i, end_i, start_j, end_j)\n",
        "\n",
        "                    # Finding the maximum value from each patch/window\n",
        "                    #print(\"pool size: \", self.pool_size)\n",
        "                    self.output[c][i][j] = self.max_value(patch, self.pool_size)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, dL_dout, lr):\n",
        "        dL_dinput = [[[0 for _ in range(self.input_width)] for _ in range(self.input_height)]for _ in range(self.num_channels)]\n",
        "\n",
        "        for c in range(self.num_channels):\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    start_i = i * self.pool_size\n",
        "                    start_j = j * self.pool_size\n",
        "\n",
        "                    end_i = start_i + self.pool_size\n",
        "                    end_j = start_j + self.pool_size\n",
        "                    patch = self.get_patch(self.input_data, c, start_i, end_i, start_j, end_j)\n",
        "\n",
        "                    mask = self.create_mask(patch)\n",
        "                    #print(\"mask:\",mask)\n",
        "                    for m in range(self.pool_size):\n",
        "                      for n in range(self.pool_size):\n",
        "                        mask[m][n] = dL_dout[c][i][j] * mask[m][n]\n",
        "                        dL_dinput[c][start_i+m][start_j+n] = mask[m][n]\n",
        "\n",
        "                    #dL_dinput[c][start_i:end_i][start_j:end_j] = mask\n",
        "                    #print(\"dl_dinput:\", dL_dinput[c][start_i:end_i][start_j:end_j])\n",
        "\n",
        "        #print(dL_dinput)\n",
        "        return dL_dinput\n",
        "\n",
        "    def get_patch(self, input_data, channel, start_i, end_i, start_j, end_j):\n",
        "        patch = self.zeros((end_i - start_i, end_j - start_j))\n",
        "        for x in range(start_i, end_i):\n",
        "            for y in range(start_j, end_j):\n",
        "                patch[x - start_i][y - start_j] = input_data[channel][x][y]\n",
        "        return patch\n",
        "\n",
        "    def max_value(self, patch, patch_size):\n",
        "        max_val = patch[0][0]\n",
        "        for x in range(patch_size):\n",
        "            for y in range(patch_size):\n",
        "                if patch[x][y] > max_val:\n",
        "                    max_val = patch[x][y]\n",
        "        return max_val\n",
        "\n",
        "    def create_mask(self, patch):\n",
        "        mask = [[0 for _ in range(self.pool_size)] for _ in range(self.pool_size)]\n",
        "        max_val = self.max_value(patch, self.pool_size)\n",
        "        for x in range(self.pool_size):\n",
        "            for y in range(self.pool_size):\n",
        "                #print(\"maskval == patch:\", patch[x][y], max_val)\n",
        "                if patch[x][y] == max_val:\n",
        "                    mask[x][y] = 1\n",
        "        return mask\n",
        "\n",
        "    def zeros(self, shape):\n",
        "        if len(shape)<3:\n",
        "          return [[0 for _ in range(shape[1])] for _ in range(shape[0])]\n",
        "        else:\n",
        "          return [[[0 for _ in range(shape[2])] for _ in range(shape[1])] for _ in range(shape[0])]\n",
        "\n",
        "    def zeros_like(self, arr):\n",
        "        shape = arr.shape\n",
        "        return self.zeros(shape)"
      ],
      "metadata": {
        "id": "dJZKGFhD7RT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Fully_Connected:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = self.random_normal1(output_size, input_size)\n",
        "        self.biases = self.random_normal2(output_size)\n",
        "\n",
        "    def random_normal1(self, shape1, sh2):\n",
        "        import random\n",
        "        return [[0.1 * (2 * random.random() - 1) for _ in range(sh2)] for _ in range(shape1)]\n",
        "\n",
        "\n",
        "    def random_normal2(self, shape):\n",
        "        import random\n",
        "        return [0.1 * (2 * random.random() - 1) for _ in range(shape)]\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_values = self.exp_values(z)\n",
        "        sum_exp_values = sum(exp_values)\n",
        "        probabilities = [val / sum_exp_values for val in exp_values]\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "    def exp_values(self, z):\n",
        "        e = 2.7183\n",
        "        exp_values = [e**val for val in z]\n",
        "        #print(exp_values)\n",
        "        return exp_values\n",
        "\n",
        "\n",
        "    def softmax_derivative(self, s):\n",
        "        s_matrix = [[0 for _ in range(len(s))] for _ in range(len(s))]\n",
        "        s_dot_st = [[0 for _ in range(len(s))] for _ in range(len(s))]\n",
        "        subtracted_matrix =  s_dot_st\n",
        "        for i in range(len(s)):\n",
        "            s_matrix[i][i] = s[i]\n",
        "\n",
        "        for i2 in range(len(s)):\n",
        "          for j in range(len(s)):\n",
        "            s_dot_st[i2][j] = s[i2] * s[j]\n",
        "\n",
        "        for z1 in range(len(s_matrix)):\n",
        "          for z2 in range(len(s_matrix[0])):\n",
        "            subtracted_matrix[z1][z2] = s_matrix[z1][z2] -  s_dot_st[z1][z2]\n",
        "\n",
        "        return subtracted_matrix\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        flattened_input = self.flatten_input(input_data)\n",
        "        self.z = self.calculate_weighted_sum(flattened_input)\n",
        "        #print(\"ZZZZ input of sooftmax:\", self.z)\n",
        "        self.output = self.softmax(self.z)\n",
        "        #print(\"output of softmax:\", self.output)\n",
        "        return self.output\n",
        "\n",
        "    def flatten_input(self, input_data):\n",
        "        #print(\"inputdate:\", len(input_data), input_data)\n",
        "        #print(\"inputShape:\", input_data.shape)\n",
        "        flattened_input = []\n",
        "        for chanel in input_data:\n",
        "          for row in chanel:\n",
        "              for val in row:\n",
        "                  #print(\"val:\",val)\n",
        "                  flattened_input.append(val)\n",
        "        #print(\"flatten data:\", flattened_input)\n",
        "        return flattened_input\n",
        "\n",
        "    def calculate_weighted_sum(self, flattened_input):\n",
        "        weighted_sum = []\n",
        "        #print(\"output size:\", self.output_size)\n",
        "        #print(\"input size:\", self.input_size)\n",
        "        #print(\"len of weight:\", len(self.weights))\n",
        "        #print(\"len of flatten:\", len(flattened_input))\n",
        "        #print(\"weights:\", self.weights)\n",
        "        for i in range(len(self.weights)):\n",
        "            sum_val = 0\n",
        "            for j in range(len(flattened_input)):\n",
        "                #print(\"i and j:\", i,j)\n",
        "                #print(\"weight:\", self.weights[i])\n",
        "                #print(\"founding erorr float: \",flattened_input[j][i+3], flattened_input[j],self.weights[i][j])\n",
        "                sum_val += self.weights[i][j] * flattened_input[j]\n",
        "            weighted_sum.append(sum_val + self.biases[i])\n",
        "        #print(\"weighted_sum:\", weighted_sum)\n",
        "        return weighted_sum\n",
        "\n",
        "    def backward(self, dL_dout, lr):\n",
        "        #print(\"dl_dout:\", dL_dout)\n",
        "        dL_dy = self.calculate_loss_gradient(dL_dout)\n",
        "        #print(\"dl_dy:\", len(dL_dy), dL_dy)\n",
        "        dL_dw = self.calculate_weight_gradient(dL_dy)\n",
        "        dL_db = dL_dy\n",
        "        dL_dinput = self.calculate_input_gradient(dL_dy)\n",
        "        self.update_weights(lr, dL_dw)\n",
        "        self.update_biases(lr, dL_db)\n",
        "        return self.reshape_input_gradient(dL_dinput)\n",
        "\n",
        "    def reshape_input_gradient(self, dL_dinput):\n",
        "      #print(\"shape of input:\", len(self.input_data), len(self.input_data[0]), len(self.input_data[0][0]))\n",
        "      din = [[[0 for _ in range(len(self.input_data[0][0]))] for _ in range(len(self.input_data[0]))] for _ in range(len(self.input_data))]\n",
        "      #print(dL_dinput)\n",
        "      for i in range(len(self.input_data)):\n",
        "        for j in range(len(self.input_data[0])):\n",
        "          for c in range(len(self.input_data[0][0])):\n",
        "            #print(j * len(self.input_data[0]) + c)\n",
        "            din[i][j][c] = dL_dinput[i][j * len(self.input_data[0]) + c]\n",
        "      return din\n",
        "\n",
        "\n",
        "    def update_biases(self, lr, dl_db):\n",
        "      #print(\"dldb:\", dl_db)\n",
        "      for i in range(len(self.biases)):\n",
        "        self.biases[i] -= lr * dl_db[i]\n",
        "\n",
        "    def update_weights(self, lr, dl_dw):\n",
        "      for i in range(self.output_size):\n",
        "        for j in range(self.input_size):\n",
        "          #print(i,j)\n",
        "          self.weights[i][j] -= lr * dl_dw[i][j]\n",
        "\n",
        "    def calculate_loss_gradient(self, dL_dout):\n",
        "        dL_dy = []\n",
        "        dout2 = []\n",
        "        for m in range(len(dL_dout)):\n",
        "          for n in range(len(dL_dout[0])):\n",
        "            dout2.append(dL_dout[m][n])\n",
        "\n",
        "        sfmmatr = self.softmax_derivative(self.output)\n",
        "        #print(\"softmax:\", sfmmatr)\n",
        "        #print(\"selfoutput:\", len(self.output), self.output)\n",
        "        #print(\"dL_dout:\", len(dL_dout), dL_dout)\n",
        "        for i in range(len(self.output)):\n",
        "            sum_val = 0\n",
        "            for j in range(len(self.output)):\n",
        "                sum_val += sfmmatr[i][j] * dout2[j]\n",
        "            dL_dy.append(sum_val)\n",
        "        #print(\"softmax_dldy:\", dL_dy)\n",
        "        return dL_dy\n",
        "\n",
        "    def calculate_weight_gradient(self, dL_dy):\n",
        "        dL_dw = [[0 for _ in range(self.input_size)] for _ in range(len(dL_dy))]\n",
        "        din = []\n",
        "        for m in self.input_data:\n",
        "          for n in m:\n",
        "            for val in n:\n",
        "              din.append(val)\n",
        "        #print(\"len of dl_dy:\", len(dL_dy))\n",
        "        #print(\"len of inputsize:\", len(din))\n",
        "        for i in range(len(dL_dy)):\n",
        "            for j in range(len(din)):\n",
        "              dL_dw[i][j] = dL_dy[i] * din[j]\n",
        "        return dL_dw\n",
        "\n",
        "    def calculate_input_gradient(self, dL_dy):\n",
        "        dL_dinput = [[0 for _ in range(self.input_size)] for _ in range(len(self.input_data))]\n",
        "        for i in range(len(dL_dy)):\n",
        "            for j in range(self.input_size):\n",
        "                dL_dinput[i // self.output_size][j] += dL_dy[i] * self.weights[i][j]\n",
        "        return dL_dinput"
      ],
      "metadata": {
        "id": "KC3ZfNRK7WKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(predictions, targets):\n",
        "    num_samples = 10\n",
        "    epsilon = 1e-7\n",
        "    # Clip predictions to avoid numerical instability\n",
        "    clipped_predictions = []\n",
        "    for p in predictions:\n",
        "        if p < epsilon:\n",
        "            clipped_predictions.append(epsilon)\n",
        "        elif p > 1 - epsilon:\n",
        "            clipped_predictions.append(1 - epsilon)\n",
        "        else:\n",
        "            clipped_predictions.append(p)\n",
        "\n",
        "    # Calculate the categorical cross-entropy loss\n",
        "    loss = 0\n",
        "    for i in range(num_samples):\n",
        "        loss -= targets[i] * ln(clipped_predictions[i])\n",
        "\n",
        "    loss /= num_samples\n",
        "\n",
        "    return loss\n",
        "\n",
        "def ln(x):\n",
        "  n=1000\n",
        "  return n*((x**(1/n))-1)\n",
        "def cross_entropy_loss_gradient(actual_labels, predicted_probs):\n",
        "    num_samples = actual_labels.shape[0]\n",
        "    epsilon = 1e-7\n",
        "    # Calculate the gradient of the cross-entropy loss function\n",
        "    gradient = []\n",
        "    for i in range(num_samples):\n",
        "        gradient.append(-actual_labels[i] / (predicted_probs[i] + epsilon) / num_samples)\n",
        "\n",
        "    return gradient\n",
        "\n",
        "def train_network(X, y, conv, pool, full, lr=0.01, epochs=20):\n",
        "    for epoch in range(epochs):\n",
        "        print(\"epoch: \", epoch)\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        for i in range(len(X)):\n",
        "            # Forward pass\n",
        "            conv_out = conv.forward(X[i])\n",
        "            pool_out = pool.forward(conv_out)\n",
        "            #print(\"pool_out:\", pool_out)\n",
        "            full_out = full.forward(pool_out)\n",
        "\n",
        "            # Calculate loss and accuracy\n",
        "            #print(\"fullout:\", full_out)\n",
        "            #print(\"yi:\", y[i])\n",
        "            loss = cross_entropy_loss(full_out, y[i])\n",
        "            total_loss += loss\n",
        "            #print(\"loss:\", total_loss, loss)\n",
        "\n",
        "            # Converting to One-Hot encoding\n",
        "            one_hot_pred = [0] * len(full_out)\n",
        "            one_hot_pred[max(range(len(full_out)), key=lambda x: full_out[x])] = 1\n",
        "            #print(\"onehot:\", one_hot_pred)\n",
        "            #one_hot_pred = flatten_list(one_hot_pred)\n",
        "\n",
        "            num_pred = max(range(len(one_hot_pred)), key=lambda x: one_hot_pred[x])\n",
        "            num_y = max(range(len(y[i])), key=lambda x: y[i][x])\n",
        "\n",
        "            if num_pred == num_y:\n",
        "                correct_predictions += 1\n",
        "\n",
        "            # Backward pass / passing the gradient of the loss function backwards\n",
        "            gradient = cross_entropy_loss_gradient(y[i], full_out)\n",
        "            #print(\"len of gradient: \",gradient)\n",
        "            #print(\"len of gradient: \", len(gradient))\n",
        "            #gradient = reshape_list(gradient, (-1, 1))\n",
        "            j = 0\n",
        "            new_grad = [[0 for _ in range(1)] for _ in range(len(gradient))]\n",
        "            #print(\"new_grad:\", new_grad)\n",
        "            for i in range(len(gradient)):\n",
        "              new_grad[i][j] = gradient[i]\n",
        "            #print(\"new_grad:\", new_grad)\n",
        "            full_back = full.backward(new_grad, lr)\n",
        "            pool_back = pool.backward(full_back, lr)\n",
        "            conv_back = conv.backward(pool_back, lr)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        average_loss = total_loss / len(X)\n",
        "        accuracy = correct_predictions / len(X) * 100.0\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f} - Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "vz2CD5sY7aoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input_sample, conv, pool, full):\n",
        "    # Forward pass through convolutional and pooling layers\n",
        "    conv_out = conv.forward(input_sample)\n",
        "    pool_out = pool.forward(conv_out)\n",
        "\n",
        "    # Flatten the output feature maps\n",
        "    flattened_output = pool_out.flatten()\n",
        "\n",
        "    # Forward pass through fully connected layer\n",
        "    predictions = full.forward(flattened_output)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "K3RMw0KL7ikF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train = train_images[:5000] / 255.0\n",
        "y_train = train_labels[:5000]\n",
        "\n",
        "X_test = train_images[5000:10000] / 255.0\n",
        "y_test = train_labels[5000:10000]\n",
        "\n",
        "X_train.shape\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "y_test[0]\n",
        "\n",
        "conv = Convolution(X_train[0].shape, 6, 1)\n",
        "input_hi, input_wi = X_train[0].shape\n",
        "print(input_hi-5)\n",
        "pool = MaxPool(2, 1, input_hi-5, input_wi-5)\n",
        "full = Fully_Connected(121, 10)\n",
        "\n",
        "train_network(X_train, y_train, conv, pool, full)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for data in X_test:\n",
        "    pred = predict(data, conv, pool, full)\n",
        "    one_hot_pred = [0] * len(pred)\n",
        "    one_hot_pred[max(range(len(pred)), key=lambda x: pred[x])] = 1\n",
        "    predictions.append(flatten_list(one_hot_pred))\n",
        "\n",
        "predictions = reshape_list(predictions, (-1, len(one_hot_pred)))\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(predictions, y_test)"
      ],
      "metadata": {
        "id": "oYM1_BkO7k8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}